{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import sys\n",
    "import os \n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from functools import partial\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from recipe_gpt.preprocessing_utilities.preprocessing_functions import (check_nan_columns, \n",
    "                                                                        extract_nutritional_numerical,\n",
    "                                                                        remove_punctuation,\n",
    "                                                                        get_files,\n",
    "                                                                        load_files, \n",
    "                                                                        replace_by_key,\n",
    "                                                                        extract_pattern, \n",
    "                                                                        remove_pattern,\n",
    "                                                                        check_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd().parent\n",
    "path = os.path.join(base_path, 'output/raw_recipes/df_final_7000_normalized_final.csv')\n",
    "df_recipes = pd.read_csv(path, index_col=0, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/victor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/victor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stop_words = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_measures = [\"bag\", \"bags\", \"liter\", \"liters\", \"bar\", \"bottle\", \"bottles\", \"bowl\", \"bowls\", \"box\", \"boxes\", \"carton\", \"jar\", \"jars\",\n",
    "                 \"cup\", \"cups\", \"drop\", \"glass\", \"piece\", \"roll\", \"rolles\", \"slice\", \"slices\", \"spoon\", \"spoons\", \"spoonful\", \"lbs\",\n",
    "                 \"all-purpose\", \"purpose\", \"diced\", \"sliced\", \"shopped\",\"oil\",\n",
    "                 \"tablespoon\", \"tablespoons\", \"large\", \"teaspoon\", \"teaspoons\", \"tube\", \"chunks\", \"chunk\", \"dice\", \"dices\", \"juice\", \"use\", \"contains\",\n",
    "                 \"contain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_names = ['united','state',  'north', 'america', 'italian', 'mediterranean', 'swiss', 'mexico', 'american', 'middle', 'eastern',\n",
    "                 'asian', 'usa', 'japan', 'indian', 'moroccan', 'cuban', 'russian', 'japanese', 'british', 'italy',  'greece', 'france',\n",
    "                 'vietnam', 'turkish', 'holland', 'lebanese', 'belgian', 'india', 'indonesian',  'chilean', 'syrian', 'venezuelan', 'ireland',\n",
    "                 'swedish', 'filipino', 'polish', 'singaporean', 'israeli', 'brazilian', 'sri', 'lanka', 'jamaican',  'finnish', 'karelian',\n",
    "                 'afghan',   'nigerian', 'egyptian',     'haitian', 'iraqi', 'maltese', 'algerian', 'canadian',  'ethiopian', 'iranian', 'malaysian',\n",
    "                 'arabic', 'norwegian', 'brazil', 'belgium', 'russia', 'egypt', 'pakistan', 'dutch', 'african', 'malaysia', 'spain', 'korea', 'lebanon',\n",
    "                 'tunisian', 'scotland', 'china', 'iran', 'hungarian',  'monterrey', 'latin', 'southern', 'persian', 'argentina', 'albanian', 'scottish',\n",
    "                 'california', 'israel', 'east',  'spanish', 'irish',  'scandinavia', 'canada',  'southeast', 'asia', 'mongolian',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_filter = ['optional', 'ripe', 'cooked', 'nutritional', 'chopped', 'cooking', 'powdered', 'serving', 'mixed', 'block','firm',\n",
    "'drained', 'pressed', 'rinsed', 'melted', 'english', 'type', 'choice', 'pound', 'crumbled','small', 'vital', 'premade', 'scramble', 'tbsp', 'finely',\n",
    "'sized', 'medium', 'ounce', 'smoke', 'peeled', 'grated', 'pre', 'made', 'based', 'granulated', 'cold', 'cubed', 'add', 'in', 'recipe', 'breakfast',\n",
    "'mashed', 'free', 'nib', 'active', 'dry', 'softened', 'packed', 'kernel', 'juiced', 'sheet', 'julienned',   'day','old', 'meal', 'tsp', 'stick',\n",
    "'star', 'inch', 'removed', 'store',  'bought', 'homemade', 'extra',  'least', 'hour', 'refrigerated', 'pocket', 'warm', 'etc', 'shred', 'thick',\n",
    "'round', 'package', 'shaving', 'plus', 'simple', 'choose','favorite', 'preferred', 'scrambled', 'additional', 'cut', 'strip','paper', 'kind','prefer',\n",
    "'can', 'overnight','frying','squash', 'apologize', 'language', 'model', 'provide','specific','trained','however','offer','general', 'substituted',\n",
    "'desired','piece', 'matcha', 'bar', 'added', 'pan', 'crushed',  'calorie','per','portion', 'mixture', 'soft', 'roughly',  'low', 'nutrition',\n",
    "'colder', 'thin', 'thinning','drizzle', 'suggested', 'energy', 'bite', 'mini', 'desiccated', 'friendly', 'half',  'serve', 'approximately', 'ingredient',\n",
    "'preparation', 'step',  'listed', 'please', 'note', 'count', 'may', 'vary', 'depending', 'size', 'brand', 'used', 'frosting', 'instant','icing', 'frothing',\n",
    "'slightly', 'stale', 'dusting', 'approx', 'adjust', 'according', 'whipped', 'heavy', 'crumb', 'includes', 'following', 'total', 'sorry', 'access',\n",
    "'database', 'tbd', 'dressing', 'quantity', 'best', 'calculate', 'unfortunately', 'ability', 'information', 'like', 'give', 'online', 'tool', 'apps',\n",
    "'text','directly', 'certainly', 'help', 'varies', 'rough', 'estimate', 'around', 'provides',  'depend', 'amount', 'hulled', 'estimated', 'button', 'fact',\n",
    "'need', 'make', 'omitted', 'along', 'still', 'real', 'time', 'unable', 'individual', 'exact', 'without', 'knowing',  'consistency', 'beaten', 'culture',\n",
    "'estimating', 'approximate', 'uncooked', 'average', 'range', 'allergic', 'warning',  'kcals', 'unknown', 'title', 'bay',  'street', 'frank', 'redhot',\n",
    "'cuisine', 'grilled', 'solid', 'start', 'using', 'grater', 'food', 'processor', 'place', 'clean', 'kitchen', 'towel', 'squeeze', 'excess', 'mixing',\n",
    "'combine', 'mix', 'well', 'evenly', 'combined', 'heat', 'skillet', 'take', 'form', 'compact', 'heated', 'cook', 'minute', 'side', 'golden', 'remove',\n",
    "'lined', 'plate', 'absorb', 'repeat', 'process', 'remaining', 'adding', 'divided', 'deep', 'undrained', 'young', 'trimmed', 'thread', 'coloring',\n",
    "'blanched', 'fried', 'instruction', 'cleaned', 'slider', 'steamed','jumbo', 'full', 'stir', 'fry', 'snap', 'textured', 'bit', 'snow', 'check',  'shape',\n",
    "'reduction', 'classic', 'sub', 'string', 'metal', 'capability', 'keep', 'mind', 'basic', 'substitute',  'part', 'prepared', 'separately', 'ripened',\n",
    "'label', 'ring', 'shortening', 'boat', 'ing', 'suitable',  'rehydrated', 'sparkling', 'fermented', 'style',  'ensure', 'required', 'regular', 'necessary',\n",
    "'allergy', 'giant', 'gigantes', 'diagonally', 'split', 'blended','mild', 'sure', 'ensure', 'required', 'regular', 'necessary', 'allergy','dish','one',\n",
    "'toothpick', 'generate', 'roasting', 'hard', 'yolk', 'everything', 'coarse', 'trail', 'ready', 'kilogram', 'combination', 'pit', 'flesh', 'mash', 'fork',\n",
    "'cover', 'plastic', 'pressing', 'onto', 'surface', 'prevent', 'browning', 'refrigerate', 'allow', 'meld', 'let','move', 'depends', 'generally', 'great',\n",
    "'top', 'marinade', 'rib', 'segmented', 'northern', 'flat', 'people', 'regional', 'influence', 'popular', 'delicious', 'lover', 'preserved', 'end', 'similar',\n",
    "'see', 'eyed', 'fine', 'century', 'semi', 'johnny', 'recommended', 'calculator', 'determine', 'value',  'traditional', 'typically', 'available', 'kcal',\n",
    "'able', 'achieve', 'room', 'temperature', 'circular', 'bubble', 'bottom', 'carefully', 'flip', 'another', 'stack', 'follows', 'precooked', 'tongue',\n",
    "'mentioned', 'replace', 'fermentation', 'center', 'tilt', 'motion', 'appear', 'edge','lift', 'cool', 'bolillos', 'quick', 'easy', 'exception', 'try',\n",
    "'parfait', 'root', 'loose', 'suggestion', 'thickness', 'fully', 'also', 'known', 'loosely', 'chop', 'could',  'quality', 'slicing', 'increase', 'assist',\n",
    "'providing', 'prepare', 'grade', 'estimation', 'would', 'data', 'caloric', 'quarter', 'provided', 'list', 'including', 'precise', 'associated','important',\n",
    "'content', 'number', 'yield', 'know', 'considered', 'segment', 'miniature', 'preheat', 'oven', 'line', 'cutter', 'resembles', 'overmix', 'turn',  'wire',\n",
    "'rack', 'follow','continue', 'imitation', 'spider', 'allergen', 'caution', 'sensitivity', 'account', 'dietary', 'restriction', 'always', 'adapt',\n",
    "'accordingly', 'included', 'different', 'none', 'confirm', 'safe', 'potential', 'trigger', 'cause', 'possible', 'intolerance', 'texture', 'sealing', 'exclude',\n",
    "'dinner', 'club', 'saturated', 'cholesterol', 'carbohydrate', 'fiber', 'factor','idea', 'covered', 'course', 'reduce', 'immersion', 'blender', 'source',\n",
    "'healthier', 'measurement', 'bomba', 'avoid', 'product', 'present', 'easily', 'adapted', 'accommodate', 'find', 'sure', 'substitution', 'common', 'sharp',\n",
    "'alternative', 'non', 'dash', 'purpose', 'difficulty', 'intermediate', 'garden', 'intolerant','avoiding', 'skip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stop_words = list_stop_words + list_measures + additional_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/victor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/victor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_nltk(text:str, stop_words:List[str]=None, lema=False, steam=False):\n",
    "  # 1 lowercase\n",
    "  new_text = text.lower()\n",
    "  new_text = re.sub(r\"(ingredient|ingredients).*:\", \" \", new_text)\n",
    "  # remove numbers\n",
    "  new_text = re.sub(r\"\\d+\", \" \", new_text)\n",
    "  # 2 Removing puntuation\n",
    "  new_text = re.sub(r'[^A-Za-z0-9 ]+', ' ', new_text)\n",
    "  # 3 Tokenization\n",
    "  words = word_tokenize(new_text)\n",
    "  # 4 Stop words filtering\n",
    "  if stop_words:\n",
    "    filtered_words = [w for w in words if not w in stop_words]\n",
    "  else:\n",
    "    filtered_words = words\n",
    "  # Filtering short words\n",
    "  filtered_words = list(filter(lambda x: len(x)>2, filtered_words))\n",
    "  if not lema and not steam:\n",
    "    return filtered_words\n",
    "  elif not lema and steam:\n",
    "    # 5 Stemming\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(w) for w in filtered_words]\n",
    "    return stemmed\n",
    "  else:\n",
    "    # 6 lematization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemas = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return lemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chickpea flour water yeast turmeric garlic powder onion powder salt pepper taste bell pepper onion tomato fresh cilantro'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_words = text_preprocessing_nltk(df_recipes[\"ingredients\"][1], stop_words=total_stop_words, lema=True)\n",
    "recipe_text = \" \".join(list_of_words)\n",
    "recipe_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_nltk = partial(text_preprocessing_nltk, stop_words=total_stop_words, lema=True)\n",
    "recipes_corpus = df_recipes[\"ingredients\"].apply(lambda x: \", \".join(processing_nltk(x)))\n",
    "list_sentences = recipes_corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes['ingredients_list'] = list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes['cuisine'].fillna('International', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_nan_columns(df_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'raw_text', 'cultural_restriction', 'calories', 'allergens',\n",
       "       'recipeId', 'ingredients', 'preparation', 'carbs', 'fat', 'fiber',\n",
       "       'protein', 'taste', 'cooking_style', 'meal_type', 'prep_time',\n",
       "       'cuisine', 'price', 'ingredients_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes.rename(columns={'title':'name',\n",
    "                         'preparation':'instructions', \n",
    "                         'carbs': 'carbohydrates'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(base_path, 'output/raw_recipes/recipes_dataset_final.csv')\n",
    "df_recipes.to_csv(out_path, sep='|', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check ingredients and preparation instruction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_check = partial(check_pattern, pattern=r'(preparation:|preparation steps:|instructions:)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df_recipes['ingredients'].apply(partial_check)\n",
    "sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df_recipes['ingredients'].str.contains('ingredients:')\n",
    "sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_extract = partial(extract_pattern, pattern=r'((preparation:|preparation steps:|instructions:)[\\s\\S]*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_instructions = df_recipes.loc[mask, 'instructions'].apply(partial_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes.loc[mask, 'instructions'] = extracted_instructions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparation:\n",
      "1. in a medium bowl, combine the diced tomatoes, red onion, jalapeno pepper, chopped cilantro, minced garlic, and lime juice.\n",
      "2. stir everything together until well mixed.\n",
      "3. taste and add salt if needed.\n",
      "4. cover the salsa and refrigerate for at least 30 minutes to allow the flavors to meld together.\n",
      "\n",
      "tortilla chips:\n",
      "ingredients:\n",
      "- corn tortillas\n",
      "- vegetable oil for frying\n",
      "- salt to taste\n",
      "\n",
      "preparation:\n",
      "1. cut the corn tortillas into triangle-shaped pieces.\n",
      "2. heat vegetable oil in a deep pot or skillet over medium heat.\n",
      "3. fry the tortilla triangles in batches until golden brown and crispy.\n",
      "4. remove the chips from the oil and place them on a paper towel-lined plate to drain excess oil.\n",
      "5. sprinkle with salt while still warm.\n",
      "6. allow the chips to cool before serving.\n",
      "\n",
      "to serve:\n",
      "1. place the guacamole and salsa in separate serving bowls.\n",
      "2. arrange the tortilla chips on a platter or individual plates.\n",
      "3. serve the guacamole and salsa alongside the tortilla chips for dipping.\n",
      "\n",
      "enjoy your guacamole and salsa with tortilla chips!\n"
     ]
    }
   ],
   "source": [
    "print(df_recipes.loc[mask, 'instructions'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'raw_text', 'cultural_restriction', 'calories', 'allergens',\n",
       "       'recipeId', 'ingredients', 'instructions', 'carbohydrates', 'fat',\n",
       "       'fiber', 'protein', 'taste', 'cooking_style', 'meal_type', 'prep_time',\n",
       "       'cuisine', 'price', 'ingredients_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes = df_recipes.drop(['meal_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_recipes.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
